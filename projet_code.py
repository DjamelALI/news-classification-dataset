# -*- coding: utf-8 -*-
"""projet_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18T8UDzv1XqwC0VWOuEtbiFyKjdoqiZ9n
"""

pip install nltk

import nltk

nltk.download('punkt')

nltk.download('stopwords')

nltk.download('wordnet')

import pandas as pd

"""# Import Data"""

from google.colab import drive
drive.mount('/content/drive')

# Read the train dataset from csv file
train_data = pd.read_csv('/content/drive/Shareddrives/Fouille de donnees - projet impairs/code/train.csv')
test_data = pd.read_csv('/content/drive/Shareddrives/Fouille de donnees - projet impairs/code/test.csv')
train_data.head()

# show dataset classes
category = list(train_data['Class Index'].unique())
category
# on a bien 4 classes :

# show the number of items in every class of training dataset
import matplotlib.pyplot as plt
train_data['Class Index'].value_counts().plot(kind='barh')
plt.show()

# Check null values
train_data.isnull().sum()

"""# 2. Text Preproccessing"""

# Text preprocessing function
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import re

# data cleaning
def clean_data(text):
  text = str(text).lower()
  text = re.sub(r"[^a-zA-Z]", " ", text)
  return text

def preprocess(text):
  # clean data
  clean_text = clean_data(text)

  # word tokenization
  tokens = word_tokenize(clean_text)

  # Remove stop words
  stop_words = stopwords.words("english")
  words = [word for word in tokens if word not in stop_words]

  # lemmatization
  lemmatizer = WordNetLemmatizer()
  lemmatized_words = [lemmatizer.lemmatize(word) for word in words]

  return " ".join(lemmatized_words)

#train_data = train_data[:1000]
train_data["Preprocessed_Text"] = (train_data['Title']+' '+train_data['Description']).apply(lambda x: preprocess(x))
test_data["Preprocessed_Text"] = (test_data['Title']+' '+test_data['Description']).apply(lambda x: preprocess(x))

train_data.head(2)

"""# Feature Extraction"""

# Extract article text and article category
categories = {1:'World News', 2:'Sports News', 3:'Business News', 4:'Science-Technology News'}

train_data['category'] = train_data['Class Index'].map(categories)
test_data['category'] = test_data['Class Index'].map(categories)

train_data = train_data.drop(columns=['Title'])
test_data = test_data.drop(columns=['Title'])

train_data = train_data.drop(columns=['Description'])
test_data = test_data.drop(columns=['Description'])

train_data = train_data.drop(columns=['Class Index'])
test_data = test_data.drop(columns=['Class Index'])

train_data.head(2)

# train data
X_train = train_data['Preprocessed_Text']
Y_train = train_data['category']

# test data
X_test = test_data['Preprocessed_Text']
Y_test = test_data['category']

# tf-idf
from sklearn.feature_extraction.text import TfidfVectorizer

tf_vec = TfidfVectorizer()
train_features = tf_vec.fit(X_train)
train_features = tf_vec.transform(X_train)
test_features = tf_vec.transform(X_test)

"""# Training the model"""

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(train_features, Y_train)

# Test the model
train_predicted = model.predict(train_features)
test_predicted = model.predict(test_features)

# Classification report
from sklearn.metrics  import classification_report

results = classification_report(Y_test, test_predicted)

# Print classifiers results
print (results)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test,test_predicted)

confusion_matrix = confusion_matrix(Y_test,test_predicted)
precision_totale = 0

for i in range(4):
  precision_totale += confusion_matrix[i][i]

precision_totale = precision_totale/7600
print(precision_totale)

precision_totale = 0
for i in range(len(test_predicted)):
  if test_predicted[i] == Y_test[i]: precision_totale += 1

precision_totale = precision_totale/7600
print(precision_totale)